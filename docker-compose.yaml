services:
  lmdeploy-internvl:
    build:
      dockerfile: ./docker/InternVL_Dockerfile
    restart: always
    image: openmmlab/lmdeploy:internvl
    # entrypoint:  ["tail", "-f", "/dev/null"]
    # cache-max-entry-count看起來是在把model load完之後，如果發現還有剩餘vram, 就會占用
    # 0.6就是占用剩下的60%, default is 0.8
    entrypoint: ["lmdeploy", "serve", "api_server", "OpenGVLab/InternVL2-1B", "--cache-max-entry-count", "0.6"]
    volumes:
      - /data/.cache:/root/.cache
      # - ./lmdeploy_docker:/opt/lmdeploy/lmdeploy
      # - ./lmdeploy/serve/openai/api_server.py:/opt/lmdeploy/lmdeploy/serve/openai/api_server.py
      # - ./lmdeploy/cli:/opt/lmdeploy/lmdeploy/cli
      # - ./lmdeploy/utils.py:/opt/lmdeploy/lmdeploy/utils.py
      - ./lmdeploy/serve/openai/protocol.py:/opt/lmdeploy/lmdeploy/serve/openai/protocol.py
      - ./lmdeploy/serve/openai/api_server.py:/opt/lmdeploy/lmdeploy/serve/openai/api_server.py

    ports:
      - "6611:23333"

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # count: 4
              capabilities: [gpu]
              device_ids: ["0"]
