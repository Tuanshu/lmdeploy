services:
  lmdeploy-internvl:
    build:
      dockerfile: ./docker/InternVL_Dockerfile
    restart: always
    image: openmmlab/lmdeploy:internvl
    # entrypoint:  ["tail", "-f", "/dev/null"]
    entrypoint: ["lmdeploy", "serve", "api_server", "OpenGVLab/InternVL2-1B"]
    volumes:
      - /data/.cache:/root/.cache
      # - ./lmdeploy_docker:/opt/lmdeploy/lmdeploy
      # - ./lmdeploy/serve/openai/api_server.py:/opt/lmdeploy/lmdeploy/serve/openai/api_server.py
      # - ./lmdeploy/cli:/opt/lmdeploy/lmdeploy/cli
      # - ./lmdeploy/utils.py:/opt/lmdeploy/lmdeploy/utils.py
      - ./lmdeploy/serve/openai/protocol.py:/opt/lmdeploy/lmdeploy/serve/openai/protocol.py

    ports:
      - "6611:23333"

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # count: 4
              capabilities: [gpu]
              device_ids: ["1"]
